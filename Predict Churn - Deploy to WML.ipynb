{"nbformat_minor": 1, "metadata": {"language_info": {"pygments_lexer": "ipython2", "name": "python", "file_extension": ".py", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 2}, "nbconvert_exporter": "python", "version": "2.7.11"}, "kernelspec": {"language": "python", "name": "python2-spark20", "display_name": "Python 2 with Spark 2.0"}}, "cells": [{"source": "<table style=\"border: none\" align=\"left\">\n   <tr style=\"border: none\">\n      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Lab: Build, Save and Deploy a Model to IBM Watson Machine Learning (WML)</b></th>\n      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n   </tr>\n</table>", "metadata": {}, "cell_type": "markdown"}, {"source": "\nThis notebook walks you through these steps:\n- Build a Spark ML model to predict customer churn\n- Save the model in the WML repository\n- Create a Deployment in WML\n- Invoke the deployed model with a REST API call to test it", "metadata": {}, "cell_type": "markdown"}, {"source": "### Step 1: Download the customer churn data", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "source": "#Run once to install the wget package\n!pip install wget", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"execution_count": null, "source": "import wget\nurl_churn='https://raw.githubusercontent.com/yfphoon/dsx_demo/master/data/customer_churn/churn.csv'\nurl_customer='https://raw.githubusercontent.com/yfphoon/dsx_demo/master/data/customer_churn/customer.csv'\n\n#remove existing files before downloading\n!rm -f churn.csv\n!rm -f customer.csv\n\nchurnFilename=wget.download(url_churn)\ncustomerFilename=wget.download(url_customer)\n\n!ls -l churn.csv\n!ls -l customer.csv", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "### Step 2: Create DataFrames with files", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\nchurn= spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"churn.csv\")\n\ncustomer = spark.read\\\n    .format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\\\n    .option(\"header\", \"true\")\\\n    .option(\"inferSchema\", \"true\")\\\n    .load(\"customer.csv\")", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "### Step 3: Merge Files", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "source": "data=customer.join(churn,customer['ID']==churn['ID']).select(customer['*'],churn['CHURN'])", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "### Step 4: Rename some columns\nThis step is to remove spaces from columns names", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "source": "data = data.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\ndata.toPandas().head()", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "### Step 5: Build the Spark pipeline and the Random Forest model\n\"Pipeline\" is an API in SparkML that's used for building models.\nAdditional information on SparkML: https://spark.apache.org/docs/2.0.2/ml-guide.html", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# StringIndexer encodes a string column of labels to a column of label indices. \nSI1 = StringIndexer(inputCol='Gender', outputCol='GenderEncoded')\nSI2 = StringIndexer(inputCol='Status',outputCol='StatusEncoded')\nSI3 = StringIndexer(inputCol='CarOwner',outputCol='CarOwnerEncoded')\nSI4 = StringIndexer(inputCol='Paymethod',outputCol='PaymethodEncoded')\nSI5 = StringIndexer(inputCol='LocalBilltype',outputCol='LocalBilltypeEncoded')\nSI6 = StringIndexer(inputCol='LongDistanceBilltype',outputCol='LongDistanceBilltypeEncoded')\n\n# Pipelines API requires that input variables are passed in  a vector\nassembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \"LocalBilltypeEncoded\", \\\n                                       \"LongDistanceBilltypeEncoded\", \"Children\", \"EstIncome\", \"Age\", \"LongDistance\", \"International\", \"Local\",\\\n                                      \"Dropped\",\"Usage\",\"RatePlan\"], outputCol=\"features\")", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"execution_count": null, "source": "# encode the label column\nlabelIndexer = StringIndexer(inputCol='CHURN', outputCol='label').fit(data)", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"execution_count": null, "source": "# instantiate the algorithm, take the default settings\nrf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"execution_count": null, "source": "# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"execution_count": null, "source": "# build the pipeline\npipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6, labelIndexer, assembler, rf, labelConverter])# Split data into train and test datasets", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"execution_count": null, "source": "# Split data into train and test datasets\n(trainingData, testingData) = data.randomSplit([0.7, 0.3],seed=9)\ntrainingData.cache()\ntestingData.cache()", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"execution_count": null, "source": "# Build model. The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages.\nmodel = pipeline.fit(trainingData)", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "### Step 6: Score the test data set", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"execution_count": null, "source": "results = model.transform(testingData)", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "### Step 7: Model Evaluation ", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "source": "print 'Precision model1 = {:.2f}.'.format(results.filter(results.label == results.prediction).count() / float(results.count()))", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"execution_count": null, "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\nprint 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(results))", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "### Step 8: Save Model in WML repository\n\nIn this section you will store your model in the Watson Machine Learning (WML) repository by using Python client libraries.\n* <a href=\"https://console.ng.bluemix.net/docs/services/PredictiveModeling/index.html\">WML Documentation</a>\n* <a href=\"http://watson-ml-api.mybluemix.net/\">WML REST API</a> \n* <a href=\"https://watson-ml-staging-libs.mybluemix.net/repository-python/\">WML Repository API</a>\n<br/>\n\nFirst, you must import client libraries.", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "source": "from repository.mlrepositoryclient import MLRepositoryClient\nfrom repository.mlrepositoryartifact import MLRepositoryArtifact", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "Put your authentication information from your instance of the Watson Machine Learning service in <a href=\"https://console.ng.bluemix.net/dashboard/apps/\" target=\"_blank\">Bluemix</a> in the next cell. You can find your information in the **Service Credentials** tab of your service instance in Bluemix.\n\n![WML Credentials](https://raw.githubusercontent.com/yfphoon/IntroToWML/master/images/WML%20Credentials.png)\n\n<span style=\"color:red\">Replace the service_path and credentials with your own information</span>\n\nservice_path=[your url]<br/>\ninstance_id=[your instance_id]<br/>\nusername=[your username]<br/>\npassword=[your password]<br/>", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "source": "# @hidden_cell\nservice_path = 'https://ibm-watson-ml.mybluemix.net'\ninstance_id = 'XXXXXX'\nusername = 'XXXXX'\npassword = 'XXXXXX'", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "Authorize the repository client:", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "source": "ml_repository_client = MLRepositoryClient(service_path)\nml_repository_client.authorize(username, password)", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "Create the model artifact.\n\n<b>Tip:</b> The MLRepositoryArtifact method expects a trained model object, training data, and a model name. (It is this model name that is displayed by the Watson Machine Learning service).\n", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "source": "model_artifact = MLRepositoryArtifact(model, training_data=trainingData, name=\"Predict Customer Churn\")", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "Save model artifact to your Watson Machine Learning instance:", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "source": "saved_model = ml_repository_client.models.save(model_artifact)", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"execution_count": null, "source": "# Print the saved model properties\nprint \"modelType: \" + saved_model.meta.prop(\"modelType\")\nprint \"creationTime: \" + str(saved_model.meta.prop(\"creationTime\"))\nprint \"modelVersionHref: \" + saved_model.meta.prop(\"modelVersionHref\")\nprint \"label: \" + saved_model.meta.prop(\"label\")", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "### Step 9: Generate the Authorization Token for Invoking the model", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "source": "import urllib3, requests, json\n\nheaders = urllib3.util.make_headers(basic_auth='{}:{}'.format(username, password))\nurl = '{}/v2/identity/token'.format(service_path)\nresponse = requests.get(url, headers=headers)\nmltoken = json.loads(response.text).get('token')", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "### Step 10:  Go to WML in Bluemix to create a Deployment Endpoint\n\n* In your <a href=\"https://console.ng.bluemix.net/dashboard/apps/\" target=\"_blank\">Bluemix</a> dashboard, click into your WML Service and click the **Launch Dashboard** button under Watson Machine Learing.\n![WML Launch Dashboard](https://raw.githubusercontent.com/yfphoon/dsx_demo/master/WML_Launch_Dashboard.png)\n\n<br/>\n* You should see your deployed model in the **Models** tab\n", "metadata": {}, "cell_type": "markdown"}, {"source": "* Under *Actions*, click on the 3 ellipses and click ***Create Deployment***.  Give your deployment configuration a unique name, e.g. \"Predict Customer Churn Deply\", select Type=Online and click **Save**.\n<br/>\n<br/>\n* In the *Deployments tab*, under *Actions*, click **View Details**\n<br/>\n<br/>\n* Scoll down to **API Details**, copy the value of the **Scoring Endpoint** into your notepad.  (e.g. \thttps://ibm-watson-ml.mybluemix.net/v2/published_models/64fd0462-3f8a-4b42-820b-59a4da9b7dc6/deployments/7d9995ed-7daf-4cfd-b40f-37cb8ab3d88f/online)", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "### Step 11:  Invoke the model through REST API call", "metadata": {}, "cell_type": "markdown"}, {"source": "#### Create a JSON Sample record for the model ", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "source": "sample_data = {\n    \"fields\": [\n    \"ID\",\n    \"Gender\",\n    \"Status\",\n    \"Children\",\n    \"EstIncome\",\n    \"CarOwner\",\n    \"Age\",\n    \"LongDistance\",\n    \"International\",\n    \"Local\",\n    \"Dropped\",\n    \"Paymethod\",\n    \"LocalBilltype\",\n    \"LongDistanceBilltype\",\n    \"Usage\",\n    \"RatePlan\"\n    ],\n    \"values\": [ [999,\"F\",\"M\",2.0,77551.100000,\"Y\",33.600000,20.530000,0.000000,41.890000,1.000000,\"CC\",\"Budget\",\"Standard\",62.420000,2.000000] ]\n} \n\nsample_json = json.dumps(sample_data)", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "#### Make Rest API call to test the deployed model", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "source": "# Get the scoring endpoint from the WML service\n# Replace the value for scoring_endpoint with your own scoring endpoint\nscoring_endpoint = 'XXXXXXX'\nheader_online = {'Content-Type': 'application/json', 'Authorization': \"Bearer \" + mltoken}\n\n# API call here\nresponse_scoring = requests.post(scoring_endpoint, data=sample_json, headers=header_online)\n\nprint response_scoring.text", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "#### Grab Predicted Value ", "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "source": "wml = json.loads(response_scoring.text)\n\n# First zip the fields and values together\nzipped_wml = zip(wml['fields'], wml['values'].pop())\n\n# Next iterate through items and grab the prediction value\nprint(\"Predicted Churn: \" + [v for (k,v) in zipped_wml if k == 'predictedLabel'].pop())", "outputs": [], "cell_type": "code", "metadata": {"collapsed": true}}, {"source": "You have come to the end of this notebook", "metadata": {"collapsed": true}, "cell_type": "markdown"}, {"source": "\n**Sidney Phoon**\n<br/>\nyfphoon@us.ibm.com\n<br/>\nSeptember 1st, 2017", "metadata": {}, "cell_type": "markdown"}], "nbformat": 4}