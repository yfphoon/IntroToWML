{"nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "<table style=\"border: none\" align=\"left\">\n   <tr style=\"border: none\">\n      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Lab: Build, Save and Deploy a Model to IBM Watson Machine Learning (WML)</b></th>\n      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n   </tr>\n</table>"}, {"cell_type": "markdown", "metadata": {}, "source": "\nThis notebook walks you through these steps:\n- Build a Spark ML model to predict customer churn\n- Save the model in the WML repository\n- Create a Deployment in WML\n- Invoke the deployed model with a REST API call to test it"}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 1: Download the customer churn data"}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "#Run once to install the wget package\n!pip install wget", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "import wget\nurl_churn='https://raw.githubusercontent.com/yfphoon/dsx_demo/master/data/customer_churn/churn.csv'\nurl_customer='https://raw.githubusercontent.com/yfphoon/dsx_demo/master/data/customer_churn/customer.csv'\n\n#remove existing files before downloading\n!rm -f churn.csv\n!rm -f customer.csv\n\nchurnFilename=wget.download(url_churn)\ncustomerFilename=wget.download(url_customer)\n\n!ls -l churn.csv\n!ls -l customer.csv", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 2: Create DataFrames with files"}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "churn= sqlContext.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(churnFilename)\ncustomer= sqlContext.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(customerFilename)", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 3: Merge Files"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "data=customer.join(churn,customer['ID']==churn['ID']).select(customer['*'],churn['CHURN'])", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 4: Rename some columns\nThis step is to remove spaces from columns names"}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "data = data.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\ndata.toPandas().head()", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 5: Build the Spark pipeline and the Random Forest model\n\"Pipeline\" is an API in SparkML that's used for building models.\nAdditional information on SparkML: https://spark.apache.org/docs/2.0.2/ml-guide.html"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# StringIndexer encodes a string column of labels to a column of label indices. \nSI1 = StringIndexer(inputCol='Gender', outputCol='GenderEncoded')\nSI2 = StringIndexer(inputCol='Status',outputCol='StatusEncoded')\nSI3 = StringIndexer(inputCol='CarOwner',outputCol='CarOwnerEncoded')\nSI4 = StringIndexer(inputCol='Paymethod',outputCol='PaymethodEncoded')\nSI5 = StringIndexer(inputCol='LocalBilltype',outputCol='LocalBilltypeEncoded')\nSI6 = StringIndexer(inputCol='LongDistanceBilltype',outputCol='LongDistanceBilltypeEncoded')\n\n# Pipelines API requires that input variables are passed in  a vector\nassembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \"LocalBilltypeEncoded\", \\\n                                       \"LongDistanceBilltypeEncoded\", \"Children\", \"EstIncome\", \"Age\", \"LongDistance\", \"International\", \"Local\",\\\n                                      \"Dropped\",\"Usage\",\"RatePlan\"], outputCol=\"features\")", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# encode the label column\nlabelIndexer = StringIndexer(inputCol='CHURN', outputCol='label').fit(data)", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# instantiate the algorithm, take the default settings\nrf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# build the pipeline\npipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6, labelIndexer, assembler, rf, labelConverter])# Split data into train and test datasets", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "# Split data into train and test datasets\n(trainingData, testingData) = data.randomSplit([0.7, 0.3],seed=9)\ntrainingData.cache()\ntestingData.cache()", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# Build model. The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages.\nmodel = pipeline.fit(trainingData)", "execution_count": null}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": "### Step 6: Score the test data set"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "results = model.transform(testingData)", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 7: Model Evaluation "}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "print 'Precision model1 = {:.2f}.'.format(results.filter(results.label == results.prediction).count() / float(results.count()))", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\nprint 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(results))", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 8: Save Model in WML repository\n\nIn this section you will store your model in the Watson Machine Learning (WML) repository by using Python client libraries.\n* <a href=\"https://console.ng.bluemix.net/docs/services/PredictiveModeling/index.html\">WML Documentation</a>\n* <a href=\"http://watson-ml-api.mybluemix.net/\">WML REST API</a> \n* <a href=\"https://watson-ml-staging-libs.mybluemix.net/repository-python/\">WML Repository API</a>\n<br/>\n\nFirst, you must import client libraries."}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "from repository.mlrepositoryclient import MLRepositoryClient\nfrom repository.mlrepositoryartifact import MLRepositoryArtifact", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "Put your authentication information from your instance of the Watson Machine Learning service in <a href=\"https://console.ng.bluemix.net/dashboard/apps/\" target=\"_blank\">Bluemix</a> in the next cell. You can find your information in the **Service Credentials** tab of your service instance in Bluemix.\n\n![WML Credentials](https://raw.githubusercontent.com/yfphoon/IntroToWML/master/images/WML%20Credentials.png)\n\n<span style=\"color:red\">Replace the service_path and credentials with your own information</span>\n\nservice_path=[your url]<br/>\ninstance_id=[your instance_id]<br/>\nusername=[your username]<br/>\npassword=[your password]<br/>"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "# @hidden_cell\nservice_path = 'https://ibm-watson-ml.mybluemix.net'\ninstance_id = 'XXXXXX'\nusername = 'XXXXX'\npassword = 'XXXXXX'", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "Authorize the repository client:"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "ml_repository_client = MLRepositoryClient(service_path)\nml_repository_client.authorize(username, password)", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "Create the model artifact.\n\n<b>Tip:</b> The MLRepositoryArtifact method expects a trained model object, training data, and a model name. (It is this model name that is displayed by the Watson Machine Learning service).\n"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "model_artifact = MLRepositoryArtifact(model, training_data=trainingData, name=\"Predict Customer Churn\")", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "Save model artifact to your Watson Machine Learning instance:"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "saved_model = ml_repository_client.models.save(model_artifact)", "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "# Print the saved model properties\nprint \"modelType: \" + saved_model.meta.prop(\"modelType\")\nprint \"creationTime: \" + str(saved_model.meta.prop(\"creationTime\"))\nprint \"modelVersionHref: \" + saved_model.meta.prop(\"modelVersionHref\")\nprint \"label: \" + saved_model.meta.prop(\"label\")", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 9: Generate the Authorization Token for Invoking the model"}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "import urllib3, requests, json\n\nheaders = urllib3.util.make_headers(basic_auth='{}:{}'.format(username, password))\nurl = '{}/v2/identity/token'.format(service_path)\nresponse = requests.get(url, headers=headers)\nmltoken = json.loads(response.text).get('token')", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 10:  Go to WML in Bluemix to create a Deployment Endpoint\n\n* In your <a href=\"https://console.ng.bluemix.net/dashboard/apps/\" target=\"_blank\">Bluemix</a> dashboard, click into your WML Service and click the **Launch Dashboard** button under Watson Machine Learing.\n![WML Launch Dashboard](https://raw.githubusercontent.com/yfphoon/dsx_demo/master/WML_Launch_Dashboard.png)\n\n<br/>\n* You should see your deployed model in the **Models** tab\n"}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": "* Under *Actions*, click on the 3 ellipses and click ***Create Deployment***.  Give your deployment configuration a unique name, e.g. \"Predict Customer Churn Deply\", select Type=Online and click **Save**.\n<br/>\n<br/>\n* In the *Deployments tab*, under *Actions*, click **View Details**\n<br/>\n<br/>\n* Scoll down to **API Details**, copy the value of the **Scoring Endpoint** into your notepad.  (e.g. \thttps://ibm-watson-ml.mybluemix.net/v2/published_models/64fd0462-3f8a-4b42-820b-59a4da9b7dc6/deployments/7d9995ed-7daf-4cfd-b40f-37cb8ab3d88f/online)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Step 11:  Invoke the model through REST API call"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Create a JSON Sample record for the model "}, {"outputs": [], "cell_type": "code", "metadata": {"collapsed": true}, "source": "sample_data = {\n    \"fields\": [\n    \"ID\",\n    \"Gender\",\n    \"Status\",\n    \"Children\",\n    \"EstIncome\",\n    \"CarOwner\",\n    \"Age\",\n    \"LongDistance\",\n    \"International\",\n    \"Local\",\n    \"Dropped\",\n    \"Paymethod\",\n    \"LocalBilltype\",\n    \"LongDistanceBilltype\",\n    \"Usage\",\n    \"RatePlan\"\n    ],\n    \"values\": [ [999,\"F\",\"M\",2.0,77551.100000,\"Y\",33.600000,20.530000,0.000000,41.890000,1.000000,\"CC\",\"Budget\",\"Standard\",62.420000,2.000000] ]\n} \n\nsample_json = json.dumps(sample_data)", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "#### Make Rest API call to test the deployed model"}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "# Get the scoring endpoint from the WML service\n# Replace the value for scoring_endpoint with your own scoring endpoint\nscoring_endpoint = 'XXXXXXX'\nheader_online = {'Content-Type': 'application/json', 'Authorization': \"Bearer \" + mltoken}\n\n# API call here\nresponse_scoring = requests.post(scoring_endpoint, data=sample_json, headers=header_online)\n\nprint response_scoring.text", "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "#### Grab Predicted Value "}, {"outputs": [], "cell_type": "code", "metadata": {}, "source": "wml = json.loads(response_scoring.text)\n\n# First zip the fields and values together\nzipped_wml = zip(wml['fields'], wml['values'].pop())\n\n# Next iterate through items and grab the prediction value\nprint(\"Predicted Churn: \" + [v for (k,v) in zipped_wml if k == 'predictedLabel'].pop())", "execution_count": null}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": "You have come to the end of this notebook"}, {"cell_type": "markdown", "metadata": {}, "source": "\n**Sidney Phoon**\n<br/>\nyfphoon@us.ibm.com\n<br/>\nSeptember 1st, 2017"}], "metadata": {"kernelspec": {"display_name": "Python 2 with Spark 2.0", "name": "python2-spark20", "language": "python"}, "language_info": {"mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 2}, "pygments_lexer": "ipython2", "version": "2.7.11", "name": "python", "file_extension": ".py", "nbconvert_exporter": "python"}}, "nbformat": 4}